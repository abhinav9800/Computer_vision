{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWDWnghbbpWd",
        "outputId": "8fc43783-e2aa-4461-c1f7-0e5d82cf15bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'boxmot'...\n",
            "remote: Enumerating objects: 13929, done.\u001b[K\n",
            "remote: Counting objects: 100% (2668/2668), done.\u001b[K\n",
            "remote: Compressing objects: 100% (914/914), done.\u001b[K\n",
            "remote: Total 13929 (delta 1894), reused 2396 (delta 1754), pack-reused 11261 (from 1)\u001b[K\n",
            "Receiving objects: 100% (13929/13929), 111.77 MiB | 24.60 MiB/s, done.\n",
            "Resolving deltas: 100% (8844/8844), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/mikel-brostrom/boxmot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23FzcO8ccAcp",
        "outputId": "619f4248-8172-4c15-94d2-73295098a1ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/boxmot\n"
          ]
        }
      ],
      "source": [
        "%cd boxmot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik18wtbAcAZH",
        "outputId": "e949ae09-750f-4ffb-e887-5c31110f0e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting boxmot\n",
            "  Downloading boxmot-10.0.82-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting filterpy<2.0.0,>=1.4.5 (from boxmot)\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/178.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0.0,>=6.1.3 (from boxmot)\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: gdown<6.0.0,>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from boxmot) (5.1.0)\n",
            "Collecting gitpython<4.0.0,>=3.1.42 (from boxmot)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lapx<0.6.0,>=0.5.5 (from boxmot)\n",
            "  Downloading lapx-0.5.10.post1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from boxmot)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from boxmot) (1.26.4)\n",
            "Requirement already satisfied: opencv-python<5.0.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from boxmot) (4.10.0.84)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from boxmot) (2.1.4)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from boxmot) (6.0.2)\n",
            "Requirement already satisfied: regex<2025.0.0,>=2024.0.0 in /usr/local/lib/python3.10/dist-packages (from boxmot) (2024.5.15)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from boxmot) (1.3.2)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from boxmot) (2.4.0+cu121)\n",
            "Collecting torchvision<0.18.0,>=0.17.1 (from boxmot)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting yacs<0.2.0,>=0.1.8 (from boxmot)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from filterpy<2.0.0,>=1.4.5->boxmot) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from filterpy<2.0.0,>=1.4.5->boxmot) (3.7.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0.0,>=6.1.3->boxmot) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown<6.0.0,>=5.1.0->boxmot) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown<6.0.0,>=5.1.0->boxmot) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown<6.0.0,>=5.1.0->boxmot) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown<6.0.0,>=5.1.0->boxmot) (4.66.5)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4.0.0,>=3.1.42->boxmot)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.0->boxmot) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.0->boxmot) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.0->boxmot) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.3.0->boxmot) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.3.0->boxmot) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (2024.6.1)\n",
            "Collecting torch<3.0.0,>=2.2.1 (from boxmot)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.18.0,>=0.17.1->boxmot) (9.4.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.42->boxmot)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.0.0->boxmot) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown<6.0.0,>=5.1.0->boxmot) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.2.1->boxmot) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.2.1->boxmot) (1.3.0)\n",
            "Downloading boxmot-10.0.82-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lapx-0.5.10.post1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110459 sha256=dd96c0dd1b138afd8ef5d27074db44089c6d409a62be8570c9178eaadf93f16e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "Successfully built filterpy\n",
            "Installing collected packages: yacs, triton, smmap, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, loguru, lapx, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, nvidia-cusolver-cu12, gitpython, filterpy, torch, torchvision, boxmot\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.22.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.22.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.22.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0+cu121\n",
            "    Uninstalling torch-2.4.0+cu121:\n",
            "      Successfully uninstalled torch-2.4.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.0+cu121\n",
            "    Uninstalling torchvision-0.19.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.19.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boxmot-10.0.82 filterpy-1.4.5 ftfy-6.2.3 gitdb-4.0.11 gitpython-3.1.43 lapx-0.5.10.post1 loguru-0.7.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 smmap-5.0.1 torch-2.2.2 torchvision-0.17.2 triton-2.2.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "! pip install boxmot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hNhTfiXcAWb",
        "outputId": "6b3908e7-d1f4-4b79-a053-8f3cec8d23cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.90-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.2)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.6-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.6.68)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.2.90-py3-none-any.whl (871 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m871.8/871.8 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.6-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.2.90 ultralytics-thop-2.0.6\n"
          ]
        }
      ],
      "source": [
        "! pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTGqLeZ4cATi"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from ultralytics.utils.plotting import Annotator, colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDqC6kfkcAOB",
        "outputId": "b8520976-3338-407c-d1a1-4cb3bb982a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.2%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torchaudio\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.4.0+cu121\n",
            "    Uninstalling torchaudio-2.4.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.4.0+cu121\n",
            "Successfully installed torchaudio-2.2.2+cu118\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsWVGAyIcAIm"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSX5URrPcAF7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5KXv9NBcADg",
        "outputId": "29a3d18a-f049-471e-892b-1ab259e0917e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Sep  8 10:34:27 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlp8g0H0cAA2",
        "outputId": "3f5a3608-6cad-49be-ea47-270976333344"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYETD0t-dgU9",
        "outputId": "b1d7f9fe-51f0-4fc6-e23f-d69d08256877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = 'cuda:0'  # First CUDA device\n",
        "else:\n",
        "    device = 'cpu'  # Fallback to CPU\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9KdYLx1L6rs"
      },
      "source": [
        "# **Tracking using DeepSort**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pMH2PWccdhFh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "from boxmot import DeepOCSORT\n",
        "\n",
        "# Function to initialize YOLO model and tracker\n",
        "def initialize_models(yolo_model_path, reid_model_path, device='cuda:0'):\n",
        "    # Load YOLO model\n",
        "    yolo_model = YOLO(yolo_model_path)\n",
        "\n",
        "    # Initialize DeepSORT tracker\n",
        "    tracker = DeepOCSORT(\n",
        "        model_weights=Path(reid_model_path),\n",
        "        device=device,\n",
        "        fp16=False,\n",
        "        max_age=360,\n",
        "        min_hits=3,\n",
        "        iou_threshold=0.15,\n",
        "        det_thresh=0.25,\n",
        "        delta_t=1,\n",
        "        asso_func=\"iou\",\n",
        "        inertia=0.7,\n",
        "        w_association_emb=0.6,\n",
        "        alpha_fixed_emb=0.95,\n",
        "        aw_param=0.5,\n",
        "        embedding_off=False\n",
        "    )\n",
        "\n",
        "    return yolo_model, tracker\n",
        "\n",
        "# Function to process individual frames\n",
        "def process_frame(frame, yolo_model, tracker, class_ids, class_names_dict):\n",
        "    # Use YOLOv8 to detect objects in the frame\n",
        "    results = yolo_model(frame)\n",
        "    dets = []\n",
        "\n",
        "    for result in results[0].boxes:\n",
        "        x1, y1, x2, y2 = result.xyxy[0].cpu().numpy()  # Move to CPU and convert to NumPy\n",
        "        conf = result.conf[0].item()  # Confidence score\n",
        "        cls = int(result.cls[0].item())  # Class label \n",
        "\n",
        "        # Filter detections to only include classes of interest\n",
        "        if cls in class_ids:\n",
        "            dets.append([x1, y1, x2, y2, conf, cls])\n",
        "\n",
        "    dets = np.array(dets) if dets else np.empty((0, 6))  # Ensure empty detections are passed in correct format\n",
        "\n",
        "    # Ensure that the array has the correct shape and values\n",
        "    if dets.size > 0:\n",
        "        tracks = tracker.update(dets, frame)\n",
        "\n",
        "        # Check if the tracker returned any tracks\n",
        "        if tracks is not None and len(tracks) > 0:\n",
        "            for track in tracks:\n",
        "                if len(track) >= 7:  # Ensure track has enough elements\n",
        "                    x1, y1, x2, y2, track_id, conf, cls = track[:7]\n",
        "                    class_name = class_names_dict.get(int(cls), \"Unknown\")  \n",
        "                    label = class_name.capitalize()  \n",
        "                    color = (0, 255, 0) if class_name == 'therapist' else (255, 0, 0)\n",
        "\n",
        "                    # Draw bounding box and label\n",
        "                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "                    cv2.putText(frame, f\"{label} {int(track_id)}\", (int(x1), int(y1) - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "        else:\n",
        "            tracker.update(None, frame)\n",
        "    else:\n",
        "        tracker.update(None, frame)\n",
        "\n",
        "    return frame\n",
        "\n",
        "# Function to save the processed video\n",
        "def save_video(input_video_path, output_video_path, yolo_model, tracker, class_ids, class_names_dict):\n",
        "    vid = cv2.VideoCapture(input_video_path)\n",
        "    fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Video writer to save the output\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = vid.read()\n",
        "        if not ret:\n",
        "            print(\"No more frames to process or error reading frame\")\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        print(f\"Processing frame {frame_count}\")\n",
        "\n",
        "        try:\n",
        "            # Process frame\n",
        "            processed_frame = process_frame(frame, yolo_model, tracker, class_ids, class_names_dict)\n",
        "            out.write(processed_frame)  # Save the frame to the output video\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing frame {frame_count}: {e}\")\n",
        "            continue  # Skip to the next frame if an error occurs\n",
        "\n",
        "    vid.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"Video processing complete. Output video saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4dqVTEsdhC1",
        "outputId": "9aaa58c4-2af0-46f9-a4ed-59b312a8f89c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-09-08 11:49:14.039\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m54\u001b[0m - \u001b[33m\u001b[1mMax age > max observations, increasing size of max observations...\u001b[0m\n",
            "\u001b[32m2024-09-08 11:49:14.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.utils.torch_utils\u001b[0m:\u001b[36mselect_device\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mYolo Tracking v10.0.81 🚀 Python-3.10.12 torch-2.2.2+cu121\n",
            "CUDA:0 (Tesla T4, 15102MiB)\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self.max_obs 365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-09-08 11:49:14.443\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m183\u001b[0m - \u001b[32m\u001b[1mLoaded pretrained weights from /content/resnet50_fc512_duke_xent.pt\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Processing frame 2598\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.2ms\n",
            "Speed: 7.9ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2599\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 19.4ms\n",
            "Speed: 2.8ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2600\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 19.4ms\n",
            "Speed: 2.6ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2601\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 13.1ms\n",
            "Speed: 8.6ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2602\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.1ms\n",
            "Speed: 8.9ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2603\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 17.1ms\n",
            "Speed: 7.0ms preprocess, 17.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2604\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 16.4ms\n",
            "Speed: 2.6ms preprocess, 16.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2605\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 2.2ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2606\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 2.6ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2607\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.6ms\n",
            "Speed: 2.4ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2608\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.2ms\n",
            "Speed: 9.8ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2609\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 7.1ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2610\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2611\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 9.1ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2612\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.9ms\n",
            "Speed: 2.5ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2613\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.2ms\n",
            "Speed: 5.9ms preprocess, 12.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2614\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2615\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2616\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 18.2ms\n",
            "Speed: 2.5ms preprocess, 18.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2617\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 16.6ms\n",
            "Speed: 3.6ms preprocess, 16.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2618\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 6.0ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2619\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 8.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2620\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 15.4ms\n",
            "Speed: 6.0ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2621\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.8ms\n",
            "Speed: 7.5ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2622\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 10.7ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2623\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 7.0ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2624\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.4ms\n",
            "Speed: 4.8ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2625\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.7ms\n",
            "Speed: 4.8ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2626\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.2ms\n",
            "Speed: 9.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2627\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.4ms\n",
            "Speed: 4.7ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2628\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 26.3ms\n",
            "Speed: 5.9ms preprocess, 26.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2629\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 21.2ms\n",
            "Speed: 5.9ms preprocess, 21.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2630\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.9ms\n",
            "Speed: 2.4ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2631\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 23.5ms\n",
            "Speed: 8.8ms preprocess, 23.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2632\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.9ms\n",
            "Speed: 8.7ms preprocess, 13.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2633\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.1ms\n",
            "Speed: 10.6ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2634\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 8.6ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2635\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.6ms\n",
            "Speed: 2.9ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2636\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.6ms\n",
            "Speed: 4.0ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2637\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2638\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.9ms\n",
            "Speed: 6.5ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2639\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2640\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.1ms\n",
            "Speed: 4.1ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2641\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 7.7ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2642\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.2ms\n",
            "Speed: 2.9ms preprocess, 14.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2643\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.6ms\n",
            "Speed: 4.0ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2644\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 20.1ms\n",
            "Speed: 3.9ms preprocess, 20.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2645\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 18.0ms\n",
            "Speed: 2.8ms preprocess, 18.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2646\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.4ms\n",
            "Speed: 6.0ms preprocess, 14.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2647\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2648\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2649\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.2ms\n",
            "Speed: 2.8ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2650\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2651\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.1ms\n",
            "Speed: 3.2ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2652\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 2.7ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2653\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 2.5ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2654\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 2.4ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2655\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2656\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.5ms\n",
            "Speed: 2.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2657\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.9ms\n",
            "Speed: 2.8ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2658\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.1ms\n",
            "Speed: 3.8ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2659\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.1ms\n",
            "Speed: 3.8ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2660\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2661\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.6ms\n",
            "Speed: 3.9ms preprocess, 13.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2662\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 7.1ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2663\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 3.0ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2664\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 3.0ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2665\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 3.0ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2666\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 2.9ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2667\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.0ms\n",
            "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2668\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.4ms\n",
            "Speed: 7.5ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2669\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2670\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.5ms\n",
            "Speed: 2.5ms preprocess, 13.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2671\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.6ms\n",
            "Speed: 2.8ms preprocess, 13.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2672\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.3ms\n",
            "Speed: 8.6ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2673\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 7.0ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2674\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.4ms\n",
            "Speed: 8.5ms preprocess, 13.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2675\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.4ms\n",
            "Speed: 3.2ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2676\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.4ms\n",
            "Speed: 7.3ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2677\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.4ms\n",
            "Speed: 3.0ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2678\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 3.0ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2679\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.8ms\n",
            "Speed: 3.0ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2680\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.6ms\n",
            "Speed: 3.0ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2681\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.5ms\n",
            "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2682\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 2.7ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2683\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.1ms\n",
            "Speed: 3.1ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2684\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.4ms\n",
            "Speed: 3.0ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2685\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.2ms\n",
            "Speed: 3.0ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2686\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2687\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 19.5ms\n",
            "Speed: 3.9ms preprocess, 19.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2688\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.1ms\n",
            "Speed: 3.8ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2689\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 3.7ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2690\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2691\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 22.6ms\n",
            "Speed: 2.5ms preprocess, 22.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2692\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.4ms\n",
            "Speed: 2.5ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2693\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 20.4ms\n",
            "Speed: 2.6ms preprocess, 20.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2694\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2695\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2696\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 15.0ms\n",
            "Speed: 10.0ms preprocess, 15.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2697\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 18.5ms\n",
            "Speed: 10.7ms preprocess, 18.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2698\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.6ms\n",
            "Speed: 5.5ms preprocess, 14.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2699\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.9ms\n",
            "Speed: 8.9ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2700\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.0ms\n",
            "Speed: 7.3ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2701\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 7.0ms preprocess, 10.7ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2702\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 20.1ms\n",
            "Speed: 4.8ms preprocess, 20.1ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2703\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 22.8ms\n",
            "Speed: 2.4ms preprocess, 22.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2704\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 15.9ms\n",
            "Speed: 8.1ms preprocess, 15.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2705\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 19.8ms\n",
            "Speed: 6.2ms preprocess, 19.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2706\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 23.6ms\n",
            "Speed: 9.3ms preprocess, 23.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2707\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2708\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 6.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2709\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 29.3ms\n",
            "Speed: 6.3ms preprocess, 29.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2710\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 26.3ms\n",
            "Speed: 4.9ms preprocess, 26.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2711\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 24.1ms\n",
            "Speed: 2.5ms preprocess, 24.1ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2712\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 29.7ms\n",
            "Speed: 3.6ms preprocess, 29.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2713\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 18.5ms\n",
            "Speed: 6.1ms preprocess, 18.5ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2714\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 29.0ms\n",
            "Speed: 2.6ms preprocess, 29.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2715\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 27.9ms\n",
            "Speed: 4.4ms preprocess, 27.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2716\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 26.2ms\n",
            "Speed: 2.6ms preprocess, 26.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2717\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 17.0ms\n",
            "Speed: 5.2ms preprocess, 17.0ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2718\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 32.2ms\n",
            "Speed: 2.5ms preprocess, 32.2ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2719\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 21.1ms\n",
            "Speed: 3.8ms preprocess, 21.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2720\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 30.4ms\n",
            "Speed: 7.7ms preprocess, 30.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2721\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 18.2ms\n",
            "Speed: 2.5ms preprocess, 18.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2722\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 15.5ms\n",
            "Speed: 5.7ms preprocess, 15.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2723\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 21.5ms\n",
            "Speed: 3.5ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2724\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 27.9ms\n",
            "Speed: 2.9ms preprocess, 27.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2725\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 23.7ms\n",
            "Speed: 8.0ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2726\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 22.1ms\n",
            "Speed: 2.5ms preprocess, 22.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2727\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 2.6ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2728\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 22.9ms\n",
            "Speed: 2.8ms preprocess, 22.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2729\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 23.7ms\n",
            "Speed: 5.3ms preprocess, 23.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2730\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.1ms\n",
            "Speed: 2.7ms preprocess, 12.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2731\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 23.6ms\n",
            "Speed: 2.6ms preprocess, 23.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2732\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 20.0ms\n",
            "Speed: 2.5ms preprocess, 20.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "An error occurred while processing frame 2732: index -2 is out of bounds for axis 0 with size 1\n",
            "Processing frame 2733\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 14.4ms\n",
            "Speed: 6.7ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "An error occurred while processing frame 2733: Unsupported 'dets' input format '<class 'NoneType'>', valid format is np.ndarray\n",
            "Processing frame 2734\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 18.7ms\n",
            "Speed: 2.5ms preprocess, 18.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2735\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 12.1ms\n",
            "Speed: 6.8ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2736\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 25.5ms\n",
            "Speed: 2.7ms preprocess, 25.5ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2737\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 17.8ms\n",
            "Speed: 2.4ms preprocess, 17.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2738\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 20.9ms\n",
            "Speed: 2.5ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2739\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 17.4ms\n",
            "Speed: 8.0ms preprocess, 17.4ms inference, 8.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2740\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 12.1ms\n",
            "Speed: 4.2ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2741\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 15.8ms\n",
            "Speed: 2.6ms preprocess, 15.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2742\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 14.0ms\n",
            "Speed: 3.2ms preprocess, 14.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2743\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 20.2ms\n",
            "Speed: 2.5ms preprocess, 20.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2744\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 16.6ms\n",
            "Speed: 4.3ms preprocess, 16.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2745\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 21.9ms\n",
            "Speed: 2.5ms preprocess, 21.9ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2746\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 28.4ms\n",
            "Speed: 2.7ms preprocess, 28.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2747\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 23.2ms\n",
            "Speed: 8.6ms preprocess, 23.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2748\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.7ms\n",
            "Speed: 9.0ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2749\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.7ms\n",
            "Speed: 6.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2750\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.7ms\n",
            "Speed: 7.6ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2751\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.7ms\n",
            "Speed: 9.4ms preprocess, 11.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2752\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 13.1ms\n",
            "Speed: 2.5ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2753\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 14.3ms\n",
            "Speed: 2.7ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2754\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 14.3ms\n",
            "Speed: 2.7ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2755\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.5ms\n",
            "Speed: 2.6ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2756\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 14.5ms\n",
            "Speed: 2.8ms preprocess, 14.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2757\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2758\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.6ms\n",
            "Speed: 2.8ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2759\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.6ms\n",
            "Speed: 3.0ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2760\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.8ms\n",
            "Speed: 3.7ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2761\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 7.3ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2762\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 22.2ms\n",
            "Speed: 2.8ms preprocess, 22.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2763\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2764\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.5ms\n",
            "Speed: 8.9ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2765\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 17.6ms\n",
            "Speed: 8.9ms preprocess, 17.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2766\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 15.4ms\n",
            "Speed: 7.4ms preprocess, 15.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2767\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 12.0ms\n",
            "Speed: 2.7ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2768\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.5ms\n",
            "Speed: 10.2ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2769\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 14.2ms\n",
            "Speed: 10.3ms preprocess, 14.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2770\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.6ms\n",
            "Speed: 9.4ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2771\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.0ms\n",
            "Speed: 5.4ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2772\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.1ms\n",
            "Speed: 3.0ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2773\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.4ms\n",
            "Speed: 3.6ms preprocess, 14.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2774\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.6ms\n",
            "Speed: 3.0ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2775\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.8ms\n",
            "Speed: 2.4ms preprocess, 18.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2776\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.5ms\n",
            "Speed: 9.5ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2777\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.4ms\n",
            "Speed: 2.7ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2778\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.7ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2779\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.6ms\n",
            "Speed: 3.4ms preprocess, 17.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2780\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.7ms\n",
            "Speed: 7.3ms preprocess, 15.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2781\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 2.7ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2782\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 2.6ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2783\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 2.6ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2784\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.9ms\n",
            "Speed: 2.6ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2785\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 2.6ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2786\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 2.8ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2787\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 3.3ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2788\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 2.7ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2789\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.0ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2790\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2791\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.1ms\n",
            "Speed: 3.8ms preprocess, 18.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2792\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.8ms\n",
            "Speed: 5.2ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2793\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 8.7ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2794\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.0ms\n",
            "Speed: 2.5ms preprocess, 24.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2795\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 16.6ms\n",
            "Speed: 2.6ms preprocess, 16.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2796\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.5ms\n",
            "Speed: 6.8ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2797\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.8ms\n",
            "Speed: 5.0ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2798\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.2ms\n",
            "Speed: 6.4ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2799\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 4.1ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2800\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.9ms\n",
            "Speed: 4.0ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2801\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 9.4ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2802\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 5.3ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2803\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.5ms\n",
            "Speed: 4.7ms preprocess, 17.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2804\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.1ms\n",
            "Speed: 8.8ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2805\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.8ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2806\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.7ms\n",
            "Speed: 2.5ms preprocess, 13.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2807\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.8ms\n",
            "Speed: 4.6ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2808\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.7ms\n",
            "Speed: 2.6ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2809\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.2ms\n",
            "Speed: 4.0ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2810\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.6ms\n",
            "Speed: 3.2ms preprocess, 15.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2811\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.7ms\n",
            "Speed: 7.3ms preprocess, 18.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2812\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 10.1ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2813\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.6ms preprocess, 10.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2814\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.9ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2815\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.3ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2816\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.4ms\n",
            "Speed: 6.8ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2817\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 3.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2818\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.5ms\n",
            "Speed: 3.0ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2819\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.6ms\n",
            "Speed: 4.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2820\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.4ms\n",
            "Speed: 3.1ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2821\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.7ms\n",
            "Speed: 2.8ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2822\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.2ms\n",
            "Speed: 3.1ms preprocess, 18.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2823\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.9ms\n",
            "Speed: 4.0ms preprocess, 21.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2824\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.9ms\n",
            "Speed: 6.8ms preprocess, 15.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2825\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 9.9ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2826\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 8.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2827\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.5ms\n",
            "Speed: 3.6ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2828\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 3.0ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2829\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 8.1ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2830\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.1ms\n",
            "Speed: 5.9ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2831\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2832\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.6ms\n",
            "Speed: 3.0ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2833\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 5.8ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2834\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 10.4ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2835\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 8.2ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2836\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.3ms\n",
            "Speed: 6.1ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2837\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.8ms\n",
            "Speed: 5.3ms preprocess, 21.8ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2838\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.5ms\n",
            "Speed: 3.0ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2839\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.3ms\n",
            "Speed: 2.5ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2840\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.2ms\n",
            "Speed: 6.6ms preprocess, 19.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2841\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.9ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2842\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2843\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.3ms\n",
            "Speed: 4.6ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2844\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.5ms\n",
            "Speed: 5.7ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2845\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 2.5ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2846\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 8.4ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2847\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.4ms\n",
            "Speed: 2.5ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2848\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.8ms\n",
            "Speed: 5.9ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2849\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.6ms\n",
            "Speed: 5.3ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2850\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.4ms\n",
            "Speed: 4.3ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2851\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.0ms\n",
            "Speed: 6.6ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2852\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.8ms\n",
            "Speed: 7.5ms preprocess, 18.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2853\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.2ms\n",
            "Speed: 2.8ms preprocess, 20.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2854\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.8ms\n",
            "Speed: 8.6ms preprocess, 19.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2855\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.8ms\n",
            "Speed: 7.0ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2856\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.6ms\n",
            "Speed: 2.6ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2857\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 2.7ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2858\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 8.3ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2859\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.6ms\n",
            "Speed: 4.1ms preprocess, 14.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2860\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2861\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.6ms\n",
            "Speed: 7.1ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2862\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 6.8ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2863\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 4.9ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2864\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2865\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.7ms\n",
            "Speed: 2.5ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2866\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2867\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.0ms\n",
            "Speed: 4.1ms preprocess, 20.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2868\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.3ms\n",
            "Speed: 3.8ms preprocess, 27.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2869\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.0ms\n",
            "Speed: 6.0ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2870\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.8ms\n",
            "Speed: 7.6ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2871\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.4ms preprocess, 10.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2872\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.1ms\n",
            "Speed: 2.5ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2873\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 2.6ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2874\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 8.5ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2875\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.3ms\n",
            "Speed: 2.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2876\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 5.0ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2877\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.9ms\n",
            "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2878\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 5.3ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2879\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 2.9ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2880\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.1ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2881\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.0ms\n",
            "Speed: 3.0ms preprocess, 19.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2882\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.9ms\n",
            "Speed: 6.8ms preprocess, 17.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2883\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.5ms\n",
            "Speed: 3.5ms preprocess, 22.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2884\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.6ms\n",
            "Speed: 4.0ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2885\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 7.6ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2886\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 6.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2887\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.2ms\n",
            "Speed: 4.2ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2888\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 12.9ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2889\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2890\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.7ms\n",
            "Speed: 3.9ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2891\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.5ms\n",
            "Speed: 3.7ms preprocess, 19.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2892\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.6ms\n",
            "Speed: 2.5ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2893\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2894\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.6ms\n",
            "Speed: 2.5ms preprocess, 20.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2895\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.1ms\n",
            "Speed: 2.8ms preprocess, 27.1ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2896\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.6ms\n",
            "Speed: 11.8ms preprocess, 27.6ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2897\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.2ms\n",
            "Speed: 7.9ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2898\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.8ms\n",
            "Speed: 4.6ms preprocess, 24.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2899\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.1ms\n",
            "Speed: 5.4ms preprocess, 19.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2900\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.8ms\n",
            "Speed: 7.3ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2901\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.7ms\n",
            "Speed: 2.5ms preprocess, 14.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2902\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.3ms\n",
            "Speed: 2.5ms preprocess, 19.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2903\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.2ms\n",
            "Speed: 2.4ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2904\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.4ms\n",
            "Speed: 4.8ms preprocess, 15.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2905\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.6ms\n",
            "Speed: 7.1ms preprocess, 17.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2906\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.4ms\n",
            "Speed: 5.3ms preprocess, 20.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2907\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.3ms\n",
            "Speed: 2.4ms preprocess, 24.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2908\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.0ms\n",
            "Speed: 2.5ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2909\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 23.8ms\n",
            "Speed: 2.7ms preprocess, 23.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2910\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.1ms\n",
            "Speed: 2.6ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2911\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.7ms\n",
            "Speed: 5.8ms preprocess, 21.7ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2912\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.3ms\n",
            "Speed: 2.6ms preprocess, 20.3ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2913\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 23.4ms\n",
            "Speed: 2.6ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2914\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.7ms\n",
            "Speed: 4.6ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2915\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.0ms\n",
            "Speed: 2.7ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2916\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.2ms\n",
            "Speed: 5.8ms preprocess, 22.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2917\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.4ms\n",
            "Speed: 4.2ms preprocess, 21.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2918\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.9ms\n",
            "Speed: 2.6ms preprocess, 13.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2919\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.0ms\n",
            "Speed: 5.7ms preprocess, 18.0ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2920\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 23.4ms\n",
            "Speed: 2.6ms preprocess, 23.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2921\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 25.6ms\n",
            "Speed: 4.6ms preprocess, 25.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2922\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2923\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.1ms\n",
            "Speed: 2.7ms preprocess, 22.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2924\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.0ms\n",
            "Speed: 2.5ms preprocess, 17.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2925\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.6ms\n",
            "Speed: 2.5ms preprocess, 27.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2926\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.5ms\n",
            "Speed: 3.1ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2927\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 16.2ms\n",
            "Speed: 3.9ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2928\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.9ms\n",
            "Speed: 2.5ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2929\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.5ms\n",
            "Speed: 3.6ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2930\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.7ms\n",
            "Speed: 3.3ms preprocess, 17.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2931\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.4ms\n",
            "Speed: 2.6ms preprocess, 20.4ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2932\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.7ms\n",
            "Speed: 2.7ms preprocess, 15.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2933\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.6ms\n",
            "Speed: 2.5ms preprocess, 27.6ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2934\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.9ms\n",
            "Speed: 7.0ms preprocess, 18.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2935\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.6ms\n",
            "Speed: 2.5ms preprocess, 24.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2936\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.4ms\n",
            "Speed: 2.5ms preprocess, 24.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2937\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 23.1ms\n",
            "Speed: 3.5ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2938\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.5ms\n",
            "Speed: 2.4ms preprocess, 22.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2939\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 31.8ms\n",
            "Speed: 3.6ms preprocess, 31.8ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2940\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 28.5ms\n",
            "Speed: 2.9ms preprocess, 28.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2941\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.3ms\n",
            "Speed: 4.9ms preprocess, 18.3ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2942\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 25.1ms\n",
            "Speed: 2.6ms preprocess, 25.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2943\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 28.4ms\n",
            "Speed: 2.5ms preprocess, 28.4ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2944\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 25.1ms\n",
            "Speed: 4.6ms preprocess, 25.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2945\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.7ms\n",
            "Speed: 2.7ms preprocess, 17.7ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2946\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.7ms\n",
            "Speed: 2.5ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2947\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.7ms\n",
            "Speed: 2.6ms preprocess, 21.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2948\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 25.5ms\n",
            "Speed: 8.1ms preprocess, 25.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2949\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.7ms\n",
            "Speed: 2.5ms preprocess, 17.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2950\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.9ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2951\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.5ms\n",
            "Speed: 11.0ms preprocess, 13.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2952\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 4.5ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2953\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.4ms\n",
            "Speed: 7.0ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2954\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.0ms\n",
            "Speed: 2.6ms preprocess, 14.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2955\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.9ms\n",
            "Speed: 2.5ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2956\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.8ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2957\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 16.1ms\n",
            "Speed: 2.6ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2958\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.5ms\n",
            "Speed: 9.0ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2959\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.2ms\n",
            "Speed: 2.4ms preprocess, 18.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2960\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.8ms\n",
            "Speed: 2.5ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2961\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.5ms\n",
            "Speed: 2.9ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2962\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.8ms\n",
            "Speed: 2.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2963\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.4ms\n",
            "Speed: 2.8ms preprocess, 14.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2964\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.6ms\n",
            "Speed: 6.8ms preprocess, 21.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2965\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.8ms\n",
            "Speed: 6.9ms preprocess, 27.8ms inference, 8.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2966\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.5ms\n",
            "Speed: 11.6ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2967\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.1ms\n",
            "Speed: 7.1ms preprocess, 14.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2968\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.6ms\n",
            "Speed: 8.4ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2969\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.4ms\n",
            "Speed: 7.7ms preprocess, 17.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2970\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 16.7ms\n",
            "Speed: 5.6ms preprocess, 16.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2971\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 9.3ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2972\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 11.5ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2973\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 8.4ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2974\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.4ms\n",
            "Speed: 9.7ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2975\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.1ms\n",
            "Speed: 5.6ms preprocess, 14.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2976\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.1ms\n",
            "Speed: 2.8ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2977\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.6ms\n",
            "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2978\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.2ms\n",
            "Speed: 2.5ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2979\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.8ms\n",
            "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2980\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.6ms\n",
            "Speed: 2.5ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2981\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 3.3ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2982\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 4.6ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2983\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.0ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2984\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 16.4ms\n",
            "Speed: 5.7ms preprocess, 16.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2985\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 5.9ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2986\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.9ms\n",
            "Speed: 3.4ms preprocess, 19.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2987\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.1ms\n",
            "Speed: 3.0ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2988\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.6ms\n",
            "Speed: 3.0ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2989\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.9ms\n",
            "Speed: 3.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2990\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.7ms\n",
            "Speed: 3.0ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2991\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2992\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.8ms\n",
            "Speed: 2.9ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2993\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.8ms\n",
            "Speed: 2.5ms preprocess, 18.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2994\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.9ms\n",
            "Speed: 16.0ms preprocess, 15.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2995\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.2ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2996\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 10.7ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2997\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.1ms\n",
            "Speed: 7.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2998\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 4.6ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 2999\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.2ms\n",
            "Speed: 5.5ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3000\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.6ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3001\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.2ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3002\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.4ms\n",
            "Speed: 9.4ms preprocess, 19.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3003\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.1ms\n",
            "Speed: 5.5ms preprocess, 14.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3004\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.2ms\n",
            "Speed: 7.8ms preprocess, 18.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3005\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.3ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3006\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 6.5ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3007\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.0ms\n",
            "Speed: 9.0ms preprocess, 19.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3008\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 16.5ms\n",
            "Speed: 6.7ms preprocess, 16.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3009\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.7ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3010\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.4ms\n",
            "Speed: 6.9ms preprocess, 15.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3011\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.7ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3012\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 16.8ms\n",
            "Speed: 6.6ms preprocess, 16.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3013\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.2ms\n",
            "Speed: 2.6ms preprocess, 19.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3014\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.9ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3015\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.8ms\n",
            "Speed: 8.8ms preprocess, 13.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3016\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 7.8ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3017\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.4ms\n",
            "Speed: 2.6ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3018\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.1ms\n",
            "Speed: 5.0ms preprocess, 18.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3019\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.7ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3020\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.8ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3021\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 23.8ms\n",
            "Speed: 9.4ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3022\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.9ms\n",
            "Speed: 9.3ms preprocess, 15.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3023\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 25.1ms\n",
            "Speed: 6.0ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3024\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 13.8ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3025\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 2.9ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3026\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 4.1ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3027\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.9ms\n",
            "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3028\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.3ms\n",
            "Speed: 2.6ms preprocess, 14.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3029\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.0ms\n",
            "Speed: 7.3ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3030\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.6ms\n",
            "Speed: 2.5ms preprocess, 20.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3031\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.9ms\n",
            "Speed: 2.5ms preprocess, 15.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3032\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 4.5ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3033\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 4.3ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3034\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.1ms\n",
            "Speed: 3.6ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3035\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.3ms\n",
            "Speed: 3.4ms preprocess, 18.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3036\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.4ms\n",
            "Speed: 3.2ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3037\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.9ms\n",
            "Speed: 2.5ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3038\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 2.4ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3039\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.5ms\n",
            "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3040\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3041\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.8ms\n",
            "Speed: 4.3ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3042\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.5ms\n",
            "Speed: 3.5ms preprocess, 14.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3043\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.0ms\n",
            "Speed: 8.6ms preprocess, 15.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3044\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.4ms\n",
            "Speed: 2.6ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3045\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.3ms\n",
            "Speed: 2.6ms preprocess, 14.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3046\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.5ms\n",
            "Speed: 2.8ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3047\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.6ms\n",
            "Speed: 2.9ms preprocess, 17.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3048\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 3.0ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3049\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 5.9ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3050\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.4ms\n",
            "Speed: 2.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3051\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.6ms\n",
            "Speed: 8.8ms preprocess, 14.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3052\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.8ms\n",
            "Speed: 10.8ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3053\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.1ms\n",
            "Speed: 6.9ms preprocess, 14.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3054\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.0ms\n",
            "Speed: 8.0ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3055\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.1ms\n",
            "Speed: 7.2ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3056\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.8ms\n",
            "Speed: 6.7ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3057\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 8.2ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3058\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.3ms\n",
            "Speed: 6.3ms preprocess, 19.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3059\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.7ms\n",
            "Speed: 5.2ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3060\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 17.9ms\n",
            "Speed: 8.0ms preprocess, 17.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3061\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.4ms\n",
            "Speed: 4.9ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3062\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.0ms\n",
            "Speed: 5.0ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3063\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.8ms\n",
            "Speed: 6.0ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3064\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.5ms\n",
            "Speed: 2.8ms preprocess, 13.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3065\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.8ms\n",
            "Speed: 2.9ms preprocess, 13.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3066\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.3ms\n",
            "Speed: 2.5ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3067\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3068\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3069\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 5.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3070\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3071\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.9ms\n",
            "Speed: 2.5ms preprocess, 14.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3072\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3073\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.7ms\n",
            "Speed: 4.7ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3074\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 2.5ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3075\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 6.8ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3076\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3077\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3078\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 2.5ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3079\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.7ms\n",
            "Speed: 2.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3080\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.9ms\n",
            "Speed: 2.6ms preprocess, 15.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3081\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.5ms\n",
            "Speed: 6.0ms preprocess, 18.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3082\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 2.7ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3083\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 2.6ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3084\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.4ms\n",
            "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3085\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.5ms\n",
            "Speed: 2.4ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3086\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 14.3ms\n",
            "Speed: 2.5ms preprocess, 14.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3087\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 5.2ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3088\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 7.7ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3089\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 2.9ms preprocess, 11.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3090\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 3.1ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3091\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.9ms\n",
            "Speed: 3.7ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3092\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.9ms\n",
            "Speed: 6.3ms preprocess, 18.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3093\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.1ms\n",
            "Speed: 6.3ms preprocess, 20.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3094\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.0ms\n",
            "Speed: 3.4ms preprocess, 15.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3095\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 29.9ms\n",
            "Speed: 2.6ms preprocess, 29.9ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3096\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3097\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.3ms\n",
            "Speed: 6.9ms preprocess, 24.3ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3098\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 31.5ms\n",
            "Speed: 2.6ms preprocess, 31.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3099\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.0ms\n",
            "Speed: 2.5ms preprocess, 18.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3100\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 25.2ms\n",
            "Speed: 2.6ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3101\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.4ms\n",
            "Speed: 5.0ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3102\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 28.4ms\n",
            "Speed: 6.8ms preprocess, 28.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3103\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 23.3ms\n",
            "Speed: 6.1ms preprocess, 23.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3104\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.0ms\n",
            "Speed: 5.8ms preprocess, 18.0ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3105\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 25.6ms\n",
            "Speed: 5.2ms preprocess, 25.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3106\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.6ms\n",
            "Speed: 2.5ms preprocess, 22.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3107\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.9ms\n",
            "Speed: 2.5ms preprocess, 20.9ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3108\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.6ms\n",
            "Speed: 2.7ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3109\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.5ms\n",
            "Speed: 2.6ms preprocess, 22.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3110\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 16.6ms\n",
            "Speed: 5.6ms preprocess, 16.6ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3111\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.3ms\n",
            "Speed: 5.7ms preprocess, 21.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3112\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.5ms\n",
            "Speed: 6.5ms preprocess, 20.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3113\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.0ms\n",
            "Speed: 5.3ms preprocess, 24.0ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3114\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.6ms\n",
            "Speed: 6.6ms preprocess, 24.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3115\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.7ms\n",
            "Speed: 2.5ms preprocess, 22.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3116\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.7ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3117\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 28.1ms\n",
            "Speed: 2.5ms preprocess, 28.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3118\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 18.7ms\n",
            "Speed: 2.5ms preprocess, 18.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3119\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.1ms\n",
            "Speed: 3.1ms preprocess, 15.1ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3120\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 25.9ms\n",
            "Speed: 7.5ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3121\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 16.0ms\n",
            "Speed: 8.8ms preprocess, 16.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3122\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.0ms\n",
            "Speed: 4.6ms preprocess, 27.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3123\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.1ms\n",
            "Speed: 5.6ms preprocess, 27.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3124\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.5ms\n",
            "Speed: 6.2ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3125\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 21.0ms\n",
            "Speed: 2.5ms preprocess, 21.0ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3126\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.1ms\n",
            "Speed: 9.7ms preprocess, 22.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3127\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 23.8ms\n",
            "Speed: 8.4ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3128\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 19.4ms\n",
            "Speed: 8.8ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3129\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 22.3ms\n",
            "Speed: 4.5ms preprocess, 22.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3130\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 23.0ms\n",
            "Speed: 2.4ms preprocess, 23.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3131\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 20.5ms\n",
            "Speed: 3.6ms preprocess, 20.5ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3132\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.8ms\n",
            "Speed: 4.4ms preprocess, 24.8ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3133\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 24.0ms\n",
            "Speed: 3.8ms preprocess, 24.0ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3134\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.4ms\n",
            "Speed: 2.6ms preprocess, 15.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3135\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.7ms\n",
            "Speed: 3.9ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3136\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.6ms\n",
            "Speed: 4.3ms preprocess, 27.6ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3137\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3138\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.5ms\n",
            "Speed: 2.5ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3139\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3140\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3141\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 3.2ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3142\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.6ms\n",
            "Speed: 2.9ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3143\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 3.0ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3144\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.0ms\n",
            "Speed: 3.5ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "An error occurred while processing frame 3144: index -2 is out of bounds for axis 0 with size 1\n",
            "Processing frame 3145\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 16.7ms\n",
            "Speed: 6.5ms preprocess, 16.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3146\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.2ms\n",
            "Speed: 2.5ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3147\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3148\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.8ms\n",
            "Speed: 2.6ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3149\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 3.1ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3150\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.1ms\n",
            "Speed: 2.9ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3151\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 7.3ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3152\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 15.2ms\n",
            "Speed: 3.3ms preprocess, 15.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3153\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.7ms\n",
            "Speed: 8.1ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3154\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.2ms\n",
            "Speed: 2.6ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3155\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 2.6ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3156\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.4ms\n",
            "Speed: 2.6ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3157\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 8.2ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3158\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.8ms\n",
            "Speed: 8.3ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3159\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 27.6ms\n",
            "Speed: 7.2ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3160\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 12.3ms\n",
            "Speed: 2.6ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3161\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 7.7ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3162\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.0ms\n",
            "Speed: 2.6ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3163\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 11.2ms\n",
            "Speed: 3.3ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3164\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.5ms\n",
            "Speed: 9.2ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3165\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.5ms\n",
            "Speed: 9.2ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3166\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 13.2ms\n",
            "Speed: 4.1ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3167\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.5ms\n",
            "Speed: 7.4ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3168\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.8ms\n",
            "Speed: 2.5ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3169\n",
            "\n",
            "0: 384x640 1 child, 1 therapist, 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3170\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 3.7ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3171\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.1ms\n",
            "Speed: 3.9ms preprocess, 14.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3172\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.0ms\n",
            "Speed: 3.0ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3173\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.0ms\n",
            "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3174\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.3ms\n",
            "Speed: 2.7ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3175\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 8.9ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3176\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 2.8ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3177\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 6.8ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3178\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.5ms\n",
            "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3179\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 3.6ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3180\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 3.0ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3181\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 16.2ms\n",
            "Speed: 2.6ms preprocess, 16.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3182\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 3.4ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3183\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.2ms\n",
            "Speed: 2.7ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3184\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.2ms\n",
            "Speed: 2.5ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3185\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.1ms\n",
            "Speed: 6.5ms preprocess, 14.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3186\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.8ms\n",
            "Speed: 10.9ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3187\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.2ms\n",
            "Speed: 7.6ms preprocess, 14.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3188\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 15.9ms\n",
            "Speed: 8.1ms preprocess, 15.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3189\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 2.7ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3190\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.1ms\n",
            "Speed: 2.9ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3191\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 15.3ms\n",
            "Speed: 11.3ms preprocess, 15.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3192\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.1ms\n",
            "Speed: 4.6ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3193\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 4.7ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3194\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.5ms\n",
            "Speed: 6.0ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3195\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.8ms\n",
            "Speed: 3.0ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3196\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 3.0ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3197\n",
            "\n",
            "0: 384x640 4 childs, 1 therapist, 10.5ms\n",
            "Speed: 3.0ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3198\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 12.6ms\n",
            "Speed: 2.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3199\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 10.5ms\n",
            "Speed: 3.1ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3200\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.2ms\n",
            "Speed: 3.0ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3201\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 14.8ms\n",
            "Speed: 3.1ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3202\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.2ms\n",
            "Speed: 3.7ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3203\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.8ms\n",
            "Speed: 2.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3204\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 4.7ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3205\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 3.2ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3206\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.0ms\n",
            "Speed: 2.4ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3207\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.4ms\n",
            "Speed: 4.2ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3208\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 10.3ms\n",
            "Speed: 8.8ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3209\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 14.7ms\n",
            "Speed: 4.9ms preprocess, 14.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3210\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 10.2ms\n",
            "Speed: 8.9ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3211\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 17.2ms\n",
            "Speed: 4.5ms preprocess, 17.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3212\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 12.9ms\n",
            "Speed: 6.3ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3213\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 17.1ms\n",
            "Speed: 10.1ms preprocess, 17.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3214\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 13.3ms\n",
            "Speed: 3.1ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3215\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 19.2ms\n",
            "Speed: 2.7ms preprocess, 19.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3216\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3217\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 2.8ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3218\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 3.0ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3219\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.2ms\n",
            "Speed: 7.0ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3220\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.2ms\n",
            "Speed: 2.8ms preprocess, 12.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3221\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 18.4ms\n",
            "Speed: 3.5ms preprocess, 18.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3222\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.6ms\n",
            "Speed: 6.3ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3223\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.6ms\n",
            "Speed: 7.0ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3224\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.4ms\n",
            "Speed: 6.4ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3225\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.8ms\n",
            "Speed: 2.9ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3226\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.8ms\n",
            "Speed: 3.5ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3227\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 3.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3228\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.8ms\n",
            "Speed: 5.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3229\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.5ms\n",
            "Speed: 3.2ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3230\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.8ms\n",
            "Speed: 8.5ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3231\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 17.3ms\n",
            "Speed: 6.0ms preprocess, 17.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3232\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3233\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.5ms\n",
            "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3234\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 8.2ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3235\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 7.8ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3236\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.5ms\n",
            "Speed: 8.9ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3237\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.9ms preprocess, 10.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3238\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 22.6ms\n",
            "Speed: 4.9ms preprocess, 22.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3239\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3240\n",
            "\n",
            "0: 384x640 4 childs, 1 therapist, 12.4ms\n",
            "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3241\n",
            "\n",
            "0: 384x640 4 childs, 1 therapist, 11.0ms\n",
            "Speed: 3.3ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3242\n",
            "\n",
            "0: 384x640 4 childs, 1 therapist, 12.4ms\n",
            "Speed: 2.8ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3243\n",
            "\n",
            "0: 384x640 4 childs, 1 therapist, 12.6ms\n",
            "Speed: 2.9ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3244\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 14.2ms\n",
            "Speed: 2.9ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3245\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 11.3ms\n",
            "Speed: 2.8ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3246\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 11.2ms\n",
            "Speed: 2.6ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3247\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 12.6ms\n",
            "Speed: 3.4ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3248\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.1ms\n",
            "Speed: 3.3ms preprocess, 15.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3249\n",
            "\n",
            "0: 384x640 2 childs, 4 therapists, 12.8ms\n",
            "Speed: 7.2ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3250\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 3.0ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3251\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.1ms\n",
            "Speed: 6.5ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3252\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 10.3ms\n",
            "Speed: 2.4ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3253\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 10.6ms\n",
            "Speed: 2.7ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3254\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 10.1ms\n",
            "Speed: 7.5ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3255\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 16.6ms\n",
            "Speed: 3.6ms preprocess, 16.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3256\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 17.5ms\n",
            "Speed: 2.7ms preprocess, 17.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3257\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.0ms\n",
            "Speed: 2.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3258\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.0ms\n",
            "Speed: 2.4ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3259\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 7.7ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3260\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.3ms\n",
            "Speed: 9.1ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3261\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.6ms\n",
            "Speed: 3.8ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3262\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.1ms\n",
            "Speed: 10.8ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3263\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 16.0ms\n",
            "Speed: 6.6ms preprocess, 16.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3264\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 10.5ms\n",
            "Speed: 2.6ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3265\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3266\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.0ms\n",
            "Speed: 2.4ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3267\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.8ms\n",
            "Speed: 3.3ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3268\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.6ms\n",
            "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3269\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.3ms\n",
            "Speed: 4.4ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3270\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.5ms\n",
            "Speed: 2.9ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3271\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3272\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.0ms\n",
            "Speed: 10.6ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3273\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.1ms\n",
            "Speed: 10.4ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3274\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.1ms\n",
            "Speed: 10.3ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3275\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.2ms\n",
            "Speed: 2.6ms preprocess, 12.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3276\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.3ms\n",
            "Speed: 2.5ms preprocess, 19.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3277\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 18.4ms\n",
            "Speed: 6.8ms preprocess, 18.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3278\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.1ms\n",
            "Speed: 2.5ms preprocess, 19.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3279\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.0ms\n",
            "Speed: 2.6ms preprocess, 23.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3280\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 24.0ms\n",
            "Speed: 3.4ms preprocess, 24.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3281\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.6ms\n",
            "Speed: 2.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3282\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.4ms\n",
            "Speed: 4.4ms preprocess, 16.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3283\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 23.2ms\n",
            "Speed: 9.5ms preprocess, 23.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3284\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 18.9ms\n",
            "Speed: 6.5ms preprocess, 18.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3285\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 24.3ms\n",
            "Speed: 3.9ms preprocess, 24.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3286\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 30.1ms\n",
            "Speed: 2.7ms preprocess, 30.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3287\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 25.4ms\n",
            "Speed: 2.8ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3288\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 36.6ms\n",
            "Speed: 2.5ms preprocess, 36.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3289\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 21.1ms\n",
            "Speed: 2.5ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3290\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 19.6ms\n",
            "Speed: 2.6ms preprocess, 19.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3291\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 20.8ms\n",
            "Speed: 2.5ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3292\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 22.6ms\n",
            "Speed: 2.7ms preprocess, 22.6ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3293\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 21.4ms\n",
            "Speed: 5.6ms preprocess, 21.4ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3294\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 30.5ms\n",
            "Speed: 2.5ms preprocess, 30.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3295\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 22.8ms\n",
            "Speed: 2.6ms preprocess, 22.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3296\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 15.7ms\n",
            "Speed: 2.5ms preprocess, 15.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3297\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.0ms\n",
            "Speed: 3.1ms preprocess, 23.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3298\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.0ms\n",
            "Speed: 3.9ms preprocess, 23.0ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3299\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 21.1ms\n",
            "Speed: 2.6ms preprocess, 21.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3300\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 21.7ms\n",
            "Speed: 2.5ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3301\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 17.0ms\n",
            "Speed: 5.9ms preprocess, 17.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3302\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 24.2ms\n",
            "Speed: 3.7ms preprocess, 24.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3303\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 13.2ms\n",
            "Speed: 2.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3304\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 26.2ms\n",
            "Speed: 6.1ms preprocess, 26.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3305\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 22.5ms\n",
            "Speed: 8.9ms preprocess, 22.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3306\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 35.4ms\n",
            "Speed: 3.8ms preprocess, 35.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3307\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 20.7ms\n",
            "Speed: 5.8ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3308\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 19.8ms\n",
            "Speed: 5.3ms preprocess, 19.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3309\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 27.1ms\n",
            "Speed: 2.5ms preprocess, 27.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3310\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 22.2ms\n",
            "Speed: 5.7ms preprocess, 22.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3311\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 26.6ms\n",
            "Speed: 6.1ms preprocess, 26.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3312\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 31.9ms\n",
            "Speed: 2.6ms preprocess, 31.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3313\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 24.5ms\n",
            "Speed: 2.6ms preprocess, 24.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3314\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 25.1ms\n",
            "Speed: 3.4ms preprocess, 25.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3315\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.7ms\n",
            "Speed: 2.5ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3316\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3317\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.2ms\n",
            "Speed: 2.6ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3318\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3319\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3320\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.6ms\n",
            "Speed: 2.5ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3321\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.2ms\n",
            "Speed: 2.5ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3322\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3323\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3324\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.9ms\n",
            "Speed: 2.5ms preprocess, 15.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3325\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.3ms\n",
            "Speed: 2.5ms preprocess, 19.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3326\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.1ms\n",
            "Speed: 8.1ms preprocess, 20.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3327\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.6ms\n",
            "Speed: 2.6ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3328\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.9ms\n",
            "Speed: 8.6ms preprocess, 15.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3329\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 7.3ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3330\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.4ms\n",
            "Speed: 5.6ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3331\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3332\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.9ms\n",
            "Speed: 3.2ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3333\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 13.7ms\n",
            "Speed: 9.5ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3334\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.3ms\n",
            "Speed: 2.8ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3335\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.6ms\n",
            "Speed: 3.2ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3336\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.6ms\n",
            "Speed: 3.1ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3337\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 23.2ms\n",
            "Speed: 3.2ms preprocess, 23.2ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3338\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.4ms\n",
            "Speed: 10.4ms preprocess, 13.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3339\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.4ms\n",
            "Speed: 2.7ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3340\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.2ms\n",
            "Speed: 2.6ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3341\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.2ms\n",
            "Speed: 2.7ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3342\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3343\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.0ms\n",
            "Speed: 10.6ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3344\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.9ms\n",
            "Speed: 4.2ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3345\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.5ms\n",
            "Speed: 4.7ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3346\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3347\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.4ms\n",
            "Speed: 5.7ms preprocess, 16.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3348\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.4ms\n",
            "Speed: 3.2ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3349\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.2ms\n",
            "Speed: 3.2ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3350\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.6ms\n",
            "Speed: 3.2ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3351\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 15.2ms\n",
            "Speed: 2.9ms preprocess, 15.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3352\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 14.9ms\n",
            "Speed: 2.8ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3353\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.5ms\n",
            "Speed: 2.7ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3354\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 11.1ms\n",
            "Speed: 4.2ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3355\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 15.8ms\n",
            "Speed: 5.5ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3356\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 10.9ms\n",
            "Speed: 3.5ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3357\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 10.5ms\n",
            "Speed: 6.5ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3358\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.8ms\n",
            "Speed: 7.3ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3359\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 18.4ms\n",
            "Speed: 7.1ms preprocess, 18.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3360\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.0ms\n",
            "Speed: 10.3ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3361\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3362\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.3ms\n",
            "Speed: 3.0ms preprocess, 16.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3363\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.5ms\n",
            "Speed: 8.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3364\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 20.6ms\n",
            "Speed: 2.6ms preprocess, 20.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3365\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 15.8ms\n",
            "Speed: 4.5ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3366\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3367\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.5ms\n",
            "Speed: 9.5ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3368\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.9ms\n",
            "Speed: 5.5ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3369\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 9.5ms\n",
            "Speed: 9.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3370\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.9ms\n",
            "Speed: 4.4ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3371\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.5ms\n",
            "Speed: 9.7ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3372\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.7ms\n",
            "Speed: 8.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3373\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.4ms\n",
            "Speed: 2.7ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3374\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3375\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.2ms\n",
            "Speed: 3.3ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3376\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.7ms\n",
            "Speed: 2.7ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3377\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.1ms\n",
            "Speed: 5.7ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3378\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.4ms\n",
            "Speed: 2.7ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3379\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 9.9ms\n",
            "Speed: 2.6ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3380\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 11.0ms\n",
            "Speed: 2.8ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3381\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.2ms\n",
            "Speed: 2.7ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3382\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.4ms\n",
            "Speed: 3.0ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3383\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 18.6ms\n",
            "Speed: 2.6ms preprocess, 18.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3384\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 9.7ms\n",
            "Speed: 7.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3385\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 14.2ms\n",
            "Speed: 6.9ms preprocess, 14.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3386\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.8ms\n",
            "Speed: 2.8ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3387\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.2ms\n",
            "Speed: 5.9ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3388\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.9ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3389\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.0ms\n",
            "Speed: 8.0ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3390\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.9ms\n",
            "Speed: 2.6ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3391\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.0ms\n",
            "Speed: 4.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3392\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 15.3ms\n",
            "Speed: 4.1ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3393\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.3ms\n",
            "Speed: 4.7ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3394\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.2ms\n",
            "Speed: 3.6ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3395\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.6ms\n",
            "Speed: 6.1ms preprocess, 16.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3396\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.3ms\n",
            "Speed: 8.8ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3397\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.0ms\n",
            "Speed: 5.7ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3398\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.3ms\n",
            "Speed: 7.3ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3399\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.3ms\n",
            "Speed: 7.1ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3400\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.3ms\n",
            "Speed: 7.5ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3401\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 17.4ms\n",
            "Speed: 5.2ms preprocess, 17.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3402\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 15.2ms\n",
            "Speed: 3.9ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3403\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 19.7ms\n",
            "Speed: 5.1ms preprocess, 19.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3404\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.5ms\n",
            "Speed: 2.7ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3405\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 20.9ms\n",
            "Speed: 4.9ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3406\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.5ms\n",
            "Speed: 6.6ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3407\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 7.4ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3408\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 7.0ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3409\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 16.2ms\n",
            "Speed: 9.0ms preprocess, 16.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3410\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 6.9ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3411\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.6ms\n",
            "Speed: 3.0ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3412\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3413\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.0ms\n",
            "Speed: 2.6ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3414\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 17.4ms\n",
            "Speed: 3.2ms preprocess, 17.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3415\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.2ms\n",
            "Speed: 7.8ms preprocess, 16.2ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3416\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.9ms\n",
            "Speed: 2.8ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3417\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 15.1ms\n",
            "Speed: 4.2ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3418\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.6ms\n",
            "Speed: 3.3ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3419\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.1ms\n",
            "Speed: 3.0ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3420\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 3.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3421\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.4ms\n",
            "Speed: 2.7ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3422\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.2ms\n",
            "Speed: 12.8ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3423\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.5ms\n",
            "Speed: 9.8ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3424\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 6.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3425\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3426\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.3ms\n",
            "Speed: 2.6ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3427\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 11.3ms\n",
            "Speed: 2.8ms preprocess, 11.3ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3428\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.9ms\n",
            "Speed: 2.7ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3429\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 8.4ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3430\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 14.3ms\n",
            "Speed: 7.2ms preprocess, 14.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3431\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.1ms\n",
            "Speed: 2.5ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3432\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 14.1ms\n",
            "Speed: 3.0ms preprocess, 14.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3433\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 15.8ms\n",
            "Speed: 2.8ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3434\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.5ms\n",
            "Speed: 10.2ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3435\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.6ms\n",
            "Speed: 2.5ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3436\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.0ms\n",
            "Speed: 2.5ms preprocess, 16.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3437\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.9ms\n",
            "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3438\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 13.2ms\n",
            "Speed: 9.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3439\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.8ms\n",
            "Speed: 10.8ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3440\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3441\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.1ms\n",
            "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3442\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.2ms\n",
            "Speed: 6.5ms preprocess, 20.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3443\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 20.8ms\n",
            "Speed: 3.2ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3444\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 23.4ms\n",
            "Speed: 5.5ms preprocess, 23.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3445\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 30.4ms\n",
            "Speed: 3.9ms preprocess, 30.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3446\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 21.4ms\n",
            "Speed: 5.9ms preprocess, 21.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3447\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 24.4ms\n",
            "Speed: 2.5ms preprocess, 24.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3448\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 26.3ms\n",
            "Speed: 4.9ms preprocess, 26.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3449\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 29.7ms\n",
            "Speed: 8.7ms preprocess, 29.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3450\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.8ms\n",
            "Speed: 2.6ms preprocess, 16.8ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3451\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 23.5ms\n",
            "Speed: 6.7ms preprocess, 23.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3452\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 25.5ms\n",
            "Speed: 2.5ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3453\n",
            "\n",
            "0: 384x640 5 childs, 2 therapists, 20.1ms\n",
            "Speed: 2.6ms preprocess, 20.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3454\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 21.0ms\n",
            "Speed: 3.3ms preprocess, 21.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3455\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 23.3ms\n",
            "Speed: 2.6ms preprocess, 23.3ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3456\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 22.0ms\n",
            "Speed: 2.6ms preprocess, 22.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3457\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 23.6ms\n",
            "Speed: 8.1ms preprocess, 23.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3458\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 15.2ms\n",
            "Speed: 4.9ms preprocess, 15.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3459\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 14.5ms\n",
            "Speed: 3.3ms preprocess, 14.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3460\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 24.8ms\n",
            "Speed: 2.5ms preprocess, 24.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3461\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 21.1ms\n",
            "Speed: 7.3ms preprocess, 21.1ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3462\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 17.1ms\n",
            "Speed: 2.7ms preprocess, 17.1ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3463\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 18.0ms\n",
            "Speed: 2.6ms preprocess, 18.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3464\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 24.5ms\n",
            "Speed: 6.4ms preprocess, 24.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3465\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 14.5ms\n",
            "Speed: 2.5ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3466\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 24.3ms\n",
            "Speed: 3.4ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3467\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.7ms\n",
            "Speed: 2.5ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3468\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.4ms\n",
            "Speed: 7.9ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3469\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 28.2ms\n",
            "Speed: 2.5ms preprocess, 28.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3470\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 15.8ms\n",
            "Speed: 2.6ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3471\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.2ms\n",
            "Speed: 3.9ms preprocess, 16.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3472\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 21.0ms\n",
            "Speed: 2.5ms preprocess, 21.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3473\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.9ms\n",
            "Speed: 7.5ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3474\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 32.5ms\n",
            "Speed: 3.1ms preprocess, 32.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3475\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 24.4ms\n",
            "Speed: 3.0ms preprocess, 24.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3476\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 20.3ms\n",
            "Speed: 4.3ms preprocess, 20.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3477\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 14.7ms\n",
            "Speed: 2.8ms preprocess, 14.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3478\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 10.7ms\n",
            "Speed: 5.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3479\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 11.2ms\n",
            "Speed: 3.3ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3480\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 15.0ms\n",
            "Speed: 4.4ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3481\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 10.3ms\n",
            "Speed: 6.3ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3482\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.7ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3483\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3484\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 21.5ms\n",
            "Speed: 4.7ms preprocess, 21.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3485\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.4ms\n",
            "Speed: 8.0ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3486\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.5ms\n",
            "Speed: 5.5ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3487\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.1ms\n",
            "Speed: 6.0ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3488\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 10.2ms\n",
            "Speed: 7.1ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3489\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.2ms\n",
            "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3490\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 10.2ms\n",
            "Speed: 8.1ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3491\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.5ms\n",
            "Speed: 6.8ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3492\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.6ms\n",
            "Speed: 7.6ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3493\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.2ms\n",
            "Speed: 7.9ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3494\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.0ms\n",
            "Speed: 6.4ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3495\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.4ms\n",
            "Speed: 6.3ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3496\n",
            "\n",
            "0: 384x640 3 childs, 4 therapists, 18.0ms\n",
            "Speed: 3.7ms preprocess, 18.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3497\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 12.4ms\n",
            "Speed: 12.6ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3498\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 10.5ms\n",
            "Speed: 7.5ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3499\n",
            "\n",
            "0: 384x640 3 childs, 4 therapists, 11.7ms\n",
            "Speed: 2.6ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3500\n",
            "\n",
            "0: 384x640 3 childs, 4 therapists, 11.1ms\n",
            "Speed: 2.8ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3501\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.7ms\n",
            "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3502\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 9.5ms\n",
            "Speed: 2.7ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3503\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.5ms\n",
            "Speed: 2.8ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3504\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 22.1ms\n",
            "Speed: 3.7ms preprocess, 22.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3505\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.0ms\n",
            "Speed: 2.6ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3506\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.7ms\n",
            "Speed: 2.6ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3507\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.8ms\n",
            "Speed: 5.0ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3508\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 17.0ms\n",
            "Speed: 5.1ms preprocess, 17.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3509\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3510\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.4ms\n",
            "Speed: 2.5ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3511\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.1ms\n",
            "Speed: 4.8ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3512\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3513\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 9.8ms\n",
            "Speed: 6.5ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3514\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 9.5ms\n",
            "Speed: 5.7ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3515\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.6ms\n",
            "Speed: 2.8ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3516\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 3.1ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3517\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.2ms\n",
            "Speed: 3.1ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3518\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 17.1ms\n",
            "Speed: 2.5ms preprocess, 17.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3519\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 9.5ms\n",
            "Speed: 6.6ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3520\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.0ms\n",
            "Speed: 5.5ms preprocess, 14.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3521\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.9ms\n",
            "Speed: 3.2ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3522\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 16.5ms\n",
            "Speed: 4.7ms preprocess, 16.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3523\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 13.3ms\n",
            "Speed: 2.5ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3524\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 2.8ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3525\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.9ms\n",
            "Speed: 2.5ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3526\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.5ms\n",
            "Speed: 5.7ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3527\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3528\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.5ms\n",
            "Speed: 2.5ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3529\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.8ms\n",
            "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3530\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.7ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3531\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 7.6ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3532\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.8ms\n",
            "Speed: 2.4ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3533\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.4ms\n",
            "Speed: 4.9ms preprocess, 19.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3534\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.6ms\n",
            "Speed: 7.5ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3535\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.8ms\n",
            "Speed: 2.5ms preprocess, 15.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3536\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.1ms\n",
            "Speed: 2.8ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3537\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 16.2ms\n",
            "Speed: 3.3ms preprocess, 16.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3538\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.1ms\n",
            "Speed: 3.2ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3539\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.6ms\n",
            "Speed: 9.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3540\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.4ms\n",
            "Speed: 4.9ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3541\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 13.0ms\n",
            "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3542\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 6.2ms preprocess, 10.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3543\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.0ms\n",
            "Speed: 6.8ms preprocess, 15.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3544\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.7ms\n",
            "Speed: 2.9ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3545\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 13.8ms\n",
            "Speed: 7.4ms preprocess, 13.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3546\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.6ms\n",
            "Speed: 4.5ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3547\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.0ms\n",
            "Speed: 2.9ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3548\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.4ms\n",
            "Speed: 2.8ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3549\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.2ms\n",
            "Speed: 2.9ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3550\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.0ms\n",
            "Speed: 2.7ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3551\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.2ms\n",
            "Speed: 2.5ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3552\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 16.2ms\n",
            "Speed: 8.4ms preprocess, 16.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3553\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.1ms\n",
            "Speed: 9.6ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3554\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.0ms\n",
            "Speed: 2.9ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3555\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.4ms\n",
            "Speed: 2.8ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3556\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 22.4ms\n",
            "Speed: 2.6ms preprocess, 22.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3557\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 10.3ms\n",
            "Speed: 2.8ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3558\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 15.0ms\n",
            "Speed: 4.6ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3559\n",
            "\n",
            "0: 384x640 4 childs, 3 therapists, 12.5ms\n",
            "Speed: 2.7ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "An error occurred while processing frame 3559: index -2 is out of bounds for axis 0 with size 1\n",
            "Processing frame 3560\n",
            "\n",
            "0: 384x640 4 childs, 3 therapists, 15.2ms\n",
            "Speed: 2.6ms preprocess, 15.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3561\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3562\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.0ms\n",
            "Speed: 2.5ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3563\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.0ms\n",
            "Speed: 8.0ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3564\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.4ms\n",
            "Speed: 10.0ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3565\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.0ms\n",
            "Speed: 4.8ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3566\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 7.7ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3567\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 11.1ms\n",
            "Speed: 6.8ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3568\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 16.5ms\n",
            "Speed: 6.5ms preprocess, 16.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3569\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 17.2ms\n",
            "Speed: 9.8ms preprocess, 17.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3570\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 7.1ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3571\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 15.7ms\n",
            "Speed: 2.6ms preprocess, 15.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3572\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.5ms\n",
            "Speed: 2.5ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3573\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 4.2ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3574\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.1ms\n",
            "Speed: 11.4ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3575\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.5ms\n",
            "Speed: 9.3ms preprocess, 11.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3576\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 7.0ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3577\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 6.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3578\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 4.7ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3579\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 6.3ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3580\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 22.0ms\n",
            "Speed: 2.7ms preprocess, 22.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3581\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.0ms\n",
            "Speed: 2.9ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3582\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 13.5ms\n",
            "Speed: 6.0ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3583\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.0ms\n",
            "Speed: 5.3ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "An error occurred while processing frame 3583: Unsupported 'dets' input format '<class 'NoneType'>', valid format is np.ndarray\n",
            "Processing frame 3584\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.3ms\n",
            "Speed: 3.0ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "An error occurred while processing frame 3584: Unsupported 'dets' input format '<class 'NoneType'>', valid format is np.ndarray\n",
            "Processing frame 3585\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.7ms\n",
            "Speed: 3.1ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3586\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.6ms\n",
            "Speed: 3.6ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3587\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.6ms\n",
            "Speed: 4.4ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3588\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.9ms\n",
            "Speed: 3.1ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3589\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.8ms\n",
            "Speed: 7.9ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3590\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.1ms\n",
            "Speed: 3.4ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3591\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.6ms\n",
            "Speed: 3.3ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3592\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 3.0ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3593\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 22.1ms\n",
            "Speed: 2.8ms preprocess, 22.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3594\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.0ms\n",
            "Speed: 5.3ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3595\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.5ms\n",
            "Speed: 5.0ms preprocess, 20.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3596\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.8ms\n",
            "Speed: 2.5ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3597\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.4ms\n",
            "Speed: 2.7ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3598\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.0ms\n",
            "Speed: 2.5ms preprocess, 20.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3599\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.5ms\n",
            "Speed: 2.6ms preprocess, 23.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3600\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.9ms\n",
            "Speed: 5.1ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3601\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 22.0ms\n",
            "Speed: 2.7ms preprocess, 22.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3602\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 26.0ms\n",
            "Speed: 6.6ms preprocess, 26.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3603\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 21.1ms\n",
            "Speed: 2.6ms preprocess, 21.1ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3604\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3605\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 36.7ms\n",
            "Speed: 3.0ms preprocess, 36.7ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3606\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.9ms\n",
            "Speed: 2.6ms preprocess, 19.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3607\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 17.9ms\n",
            "Speed: 3.1ms preprocess, 17.9ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3608\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 22.1ms\n",
            "Speed: 2.8ms preprocess, 22.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3609\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 18.1ms\n",
            "Speed: 5.4ms preprocess, 18.1ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3610\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 26.7ms\n",
            "Speed: 3.7ms preprocess, 26.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3611\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 22.6ms\n",
            "Speed: 3.1ms preprocess, 22.6ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3612\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 17.3ms\n",
            "Speed: 2.4ms preprocess, 17.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3613\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 37.5ms\n",
            "Speed: 8.0ms preprocess, 37.5ms inference, 9.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3614\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 34.1ms\n",
            "Speed: 2.7ms preprocess, 34.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3615\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.0ms\n",
            "Speed: 2.6ms preprocess, 20.0ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3616\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.5ms\n",
            "Speed: 2.7ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3617\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 13.8ms\n",
            "Speed: 6.9ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3618\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 27.3ms\n",
            "Speed: 2.6ms preprocess, 27.3ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3619\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.3ms\n",
            "Speed: 4.9ms preprocess, 23.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3620\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 28.2ms\n",
            "Speed: 3.9ms preprocess, 28.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3621\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 27.6ms\n",
            "Speed: 2.6ms preprocess, 27.6ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3622\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 21.6ms\n",
            "Speed: 4.2ms preprocess, 21.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3623\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.4ms\n",
            "Speed: 3.8ms preprocess, 12.4ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3624\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 24.4ms\n",
            "Speed: 2.9ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3625\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 18.2ms\n",
            "Speed: 5.6ms preprocess, 18.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3626\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.4ms\n",
            "Speed: 2.7ms preprocess, 23.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3627\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 30.7ms\n",
            "Speed: 2.6ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3628\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 30.6ms\n",
            "Speed: 2.5ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3629\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.7ms\n",
            "Speed: 5.7ms preprocess, 23.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3630\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 39.2ms\n",
            "Speed: 5.7ms preprocess, 39.2ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3631\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.0ms\n",
            "Speed: 6.6ms preprocess, 23.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3632\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 30.6ms\n",
            "Speed: 7.6ms preprocess, 30.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3633\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 39.2ms\n",
            "Speed: 4.5ms preprocess, 39.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3634\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 16.9ms\n",
            "Speed: 4.2ms preprocess, 16.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3635\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.8ms\n",
            "Speed: 2.9ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3636\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.2ms\n",
            "Speed: 8.6ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3637\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 13.0ms\n",
            "Speed: 2.5ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3638\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3639\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 11.0ms\n",
            "Speed: 2.7ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3640\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 11.3ms\n",
            "Speed: 3.3ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3641\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 17.5ms\n",
            "Speed: 2.9ms preprocess, 17.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3642\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 11.2ms\n",
            "Speed: 2.8ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3643\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.4ms\n",
            "Speed: 2.9ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3644\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.1ms\n",
            "Speed: 2.8ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3645\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.5ms\n",
            "Speed: 2.5ms preprocess, 11.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3646\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.1ms\n",
            "Speed: 6.8ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3647\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.9ms\n",
            "Speed: 2.5ms preprocess, 13.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3648\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.5ms\n",
            "Speed: 2.8ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3649\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 17.8ms\n",
            "Speed: 8.4ms preprocess, 17.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3650\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.1ms\n",
            "Speed: 6.7ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3651\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 18.0ms\n",
            "Speed: 12.2ms preprocess, 18.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3652\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.1ms\n",
            "Speed: 2.6ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3653\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.6ms\n",
            "Speed: 2.8ms preprocess, 12.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3654\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.6ms\n",
            "Speed: 4.2ms preprocess, 12.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3655\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.1ms\n",
            "Speed: 2.6ms preprocess, 12.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3656\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.1ms\n",
            "Speed: 4.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3657\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.5ms\n",
            "Speed: 7.3ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3658\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 9.9ms\n",
            "Speed: 5.3ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3659\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 14.1ms\n",
            "Speed: 4.0ms preprocess, 14.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3660\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.6ms\n",
            "Speed: 3.4ms preprocess, 19.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3661\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.5ms\n",
            "Speed: 2.6ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3662\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3663\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 23.0ms\n",
            "Speed: 2.6ms preprocess, 23.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3664\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 9.9ms\n",
            "Speed: 8.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3665\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 3.7ms preprocess, 11.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3666\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 20.0ms\n",
            "Speed: 4.3ms preprocess, 20.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3667\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.9ms\n",
            "Speed: 4.9ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3668\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 11.2ms\n",
            "Speed: 4.2ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3669\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 11.1ms\n",
            "Speed: 8.1ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3670\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 16.1ms\n",
            "Speed: 3.5ms preprocess, 16.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3671\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 17.0ms\n",
            "Speed: 2.4ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3672\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.1ms\n",
            "Speed: 2.5ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3673\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.9ms\n",
            "Speed: 2.9ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3674\n",
            "\n",
            "0: 384x640 3 childs, 1 therapist, 11.0ms\n",
            "Speed: 3.7ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3675\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.3ms\n",
            "Speed: 3.0ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3676\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.1ms\n",
            "Speed: 3.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3677\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 3.1ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3678\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.7ms\n",
            "Speed: 3.3ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3679\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 14.4ms\n",
            "Speed: 5.9ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3680\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 13.2ms\n",
            "Speed: 2.5ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3681\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.8ms\n",
            "Speed: 4.0ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3682\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 10.7ms\n",
            "Speed: 2.8ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3683\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.0ms\n",
            "Speed: 2.6ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3684\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.2ms\n",
            "Speed: 2.5ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3685\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 5.4ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3686\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 21.7ms\n",
            "Speed: 5.8ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3687\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.8ms\n",
            "Speed: 2.7ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3688\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.0ms\n",
            "Speed: 2.7ms preprocess, 19.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3689\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 7.1ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3690\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 8.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3691\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 17.5ms\n",
            "Speed: 5.5ms preprocess, 17.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3692\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.6ms\n",
            "Speed: 8.5ms preprocess, 15.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3693\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3694\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.1ms\n",
            "Speed: 7.4ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3695\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.6ms\n",
            "Speed: 3.7ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3696\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.8ms\n",
            "Speed: 8.6ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3697\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.6ms\n",
            "Speed: 2.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3698\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 24.9ms\n",
            "Speed: 3.4ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3699\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3700\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 17.4ms\n",
            "Speed: 10.7ms preprocess, 17.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3701\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.7ms\n",
            "Speed: 2.7ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3702\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 17.9ms\n",
            "Speed: 3.9ms preprocess, 17.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3703\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.4ms\n",
            "Speed: 3.0ms preprocess, 15.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3704\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.7ms\n",
            "Speed: 11.6ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3705\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.6ms\n",
            "Speed: 2.8ms preprocess, 15.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3706\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 2.7ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3707\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.1ms\n",
            "Speed: 2.6ms preprocess, 14.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3708\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.1ms\n",
            "Speed: 2.6ms preprocess, 11.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3709\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.1ms\n",
            "Speed: 2.6ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3710\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.9ms\n",
            "Speed: 5.9ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3711\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.1ms\n",
            "Speed: 5.5ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3712\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.8ms\n",
            "Speed: 2.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3713\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3714\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 13.9ms\n",
            "Speed: 3.4ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3715\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 17.1ms\n",
            "Speed: 5.6ms preprocess, 17.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3716\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 10.0ms\n",
            "Speed: 3.0ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3717\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 26.0ms\n",
            "Speed: 3.5ms preprocess, 26.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3718\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 18.3ms\n",
            "Speed: 5.7ms preprocess, 18.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3719\n",
            "\n",
            "0: 384x640 2 childs, 4 therapists, 11.4ms\n",
            "Speed: 3.7ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3720\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 10.3ms\n",
            "Speed: 3.1ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3721\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 25.3ms\n",
            "Speed: 7.0ms preprocess, 25.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3722\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.6ms\n",
            "Speed: 2.8ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3723\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.8ms\n",
            "Speed: 3.0ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3724\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.3ms\n",
            "Speed: 3.2ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3725\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 24.7ms\n",
            "Speed: 7.1ms preprocess, 24.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3726\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.0ms\n",
            "Speed: 3.1ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3727\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 13.5ms\n",
            "Speed: 2.5ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3728\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3729\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 12.8ms\n",
            "Speed: 3.2ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3730\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.4ms\n",
            "Speed: 3.2ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3731\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.5ms\n",
            "Speed: 3.1ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3732\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.8ms\n",
            "Speed: 3.0ms preprocess, 10.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3733\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.6ms\n",
            "Speed: 2.8ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3734\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 10.7ms\n",
            "Speed: 5.7ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3735\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 11.8ms\n",
            "Speed: 3.1ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3736\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 20.4ms\n",
            "Speed: 3.7ms preprocess, 20.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3737\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 20.3ms\n",
            "Speed: 7.5ms preprocess, 20.3ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3738\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.5ms\n",
            "Speed: 3.1ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3739\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.0ms\n",
            "Speed: 2.7ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3740\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.4ms\n",
            "Speed: 3.2ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3741\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.4ms\n",
            "Speed: 4.6ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3742\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.2ms\n",
            "Speed: 4.6ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3743\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.6ms\n",
            "Speed: 3.6ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3744\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.9ms\n",
            "Speed: 7.7ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3745\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.7ms\n",
            "Speed: 8.0ms preprocess, 11.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3746\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 10.8ms\n",
            "Speed: 2.6ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3747\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 13.8ms\n",
            "Speed: 5.6ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3748\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.9ms\n",
            "Speed: 3.0ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3749\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 12.8ms\n",
            "Speed: 3.2ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3750\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.0ms\n",
            "Speed: 2.9ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3751\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 11.6ms\n",
            "Speed: 3.2ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3752\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 11.0ms\n",
            "Speed: 2.6ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3753\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3754\n",
            "\n",
            "0: 384x640 4 childs, 2 therapists, 13.0ms\n",
            "Speed: 2.6ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3755\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.9ms\n",
            "Speed: 3.6ms preprocess, 11.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3756\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.4ms\n",
            "Speed: 2.5ms preprocess, 23.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3757\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 30.4ms\n",
            "Speed: 6.5ms preprocess, 30.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3758\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 13.1ms\n",
            "Speed: 2.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3759\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.0ms\n",
            "Speed: 4.5ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3760\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.2ms\n",
            "Speed: 2.6ms preprocess, 14.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3761\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.0ms\n",
            "Speed: 3.3ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3762\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.2ms\n",
            "Speed: 5.2ms preprocess, 14.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3763\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 26.0ms\n",
            "Speed: 5.6ms preprocess, 26.0ms inference, 12.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3764\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.0ms\n",
            "Speed: 2.6ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3765\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 18.7ms\n",
            "Speed: 4.4ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3766\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 27.4ms\n",
            "Speed: 2.6ms preprocess, 27.4ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3767\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.9ms\n",
            "Speed: 8.3ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3768\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.6ms\n",
            "Speed: 6.7ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3769\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.3ms\n",
            "Speed: 4.4ms preprocess, 23.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3770\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.7ms\n",
            "Speed: 6.5ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3771\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.2ms\n",
            "Speed: 4.7ms preprocess, 15.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3772\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 14.9ms\n",
            "Speed: 6.7ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3773\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.6ms\n",
            "Speed: 7.4ms preprocess, 19.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3774\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 21.7ms\n",
            "Speed: 5.9ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3775\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 30.5ms\n",
            "Speed: 5.0ms preprocess, 30.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3776\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.4ms\n",
            "Speed: 4.6ms preprocess, 19.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3777\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 20.5ms\n",
            "Speed: 8.1ms preprocess, 20.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3778\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 15.8ms\n",
            "Speed: 9.1ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3779\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.9ms\n",
            "Speed: 8.1ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3780\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 13.5ms\n",
            "Speed: 7.0ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3781\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 19.4ms\n",
            "Speed: 4.5ms preprocess, 19.4ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3782\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 29.7ms\n",
            "Speed: 3.6ms preprocess, 29.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3783\n",
            "\n",
            "0: 384x640 3 childs, 3 therapists, 22.1ms\n",
            "Speed: 2.6ms preprocess, 22.1ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3784\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 26.7ms\n",
            "Speed: 3.6ms preprocess, 26.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3785\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 23.5ms\n",
            "Speed: 5.5ms preprocess, 23.5ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3786\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.9ms\n",
            "Speed: 3.4ms preprocess, 20.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3787\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 20.2ms\n",
            "Speed: 2.7ms preprocess, 20.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3788\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 19.1ms\n",
            "Speed: 2.7ms preprocess, 19.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3789\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 23.5ms\n",
            "Speed: 4.0ms preprocess, 23.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3790\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 19.7ms\n",
            "Speed: 2.9ms preprocess, 19.7ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3791\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 22.7ms\n",
            "Speed: 2.7ms preprocess, 22.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3792\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 36.9ms\n",
            "Speed: 5.3ms preprocess, 36.9ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3793\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 16.9ms\n",
            "Speed: 2.6ms preprocess, 16.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3794\n",
            "\n",
            "0: 384x640 2 childs, 4 therapists, 11.3ms\n",
            "Speed: 2.7ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3795\n",
            "\n",
            "0: 384x640 2 childs, 3 therapists, 23.5ms\n",
            "Speed: 2.7ms preprocess, 23.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3796\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 17.1ms\n",
            "Speed: 8.3ms preprocess, 17.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3797\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 25.1ms\n",
            "Speed: 4.7ms preprocess, 25.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3798\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 18.1ms\n",
            "Speed: 6.3ms preprocess, 18.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3799\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 13.7ms\n",
            "Speed: 9.0ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3800\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.2ms\n",
            "Speed: 2.8ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3801\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3802\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 12.3ms\n",
            "Speed: 2.8ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3803\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 14.1ms\n",
            "Speed: 3.8ms preprocess, 14.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3804\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 14.3ms\n",
            "Speed: 5.2ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3805\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 11.3ms\n",
            "Speed: 8.0ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3806\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 11.4ms\n",
            "Speed: 6.6ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3807\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 11.4ms\n",
            "Speed: 7.0ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3808\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 11.6ms\n",
            "Speed: 6.1ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3809\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 10.7ms\n",
            "Speed: 9.4ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3810\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 10.7ms\n",
            "Speed: 8.7ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3811\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 10.8ms\n",
            "Speed: 6.4ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3812\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.9ms\n",
            "Speed: 2.7ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3813\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 12.4ms\n",
            "Speed: 5.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3814\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 10.8ms\n",
            "Speed: 2.7ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3815\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 14.1ms\n",
            "Speed: 4.5ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3816\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 20.6ms\n",
            "Speed: 3.2ms preprocess, 20.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3817\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 23.9ms\n",
            "Speed: 2.7ms preprocess, 23.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3818\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.7ms\n",
            "Speed: 4.2ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3819\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 11.5ms\n",
            "Speed: 3.0ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3820\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 12.1ms\n",
            "Speed: 2.6ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3821\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 11.1ms\n",
            "Speed: 3.2ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3822\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 11.3ms\n",
            "Speed: 3.2ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3823\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.3ms\n",
            "Speed: 3.1ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3824\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.3ms\n",
            "Speed: 3.2ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3825\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.3ms\n",
            "Speed: 2.6ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3826\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.3ms\n",
            "Speed: 2.9ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3827\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.0ms\n",
            "Speed: 2.9ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3828\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 21.2ms\n",
            "Speed: 5.4ms preprocess, 21.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3829\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 14.8ms\n",
            "Speed: 5.4ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3830\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 11.5ms\n",
            "Speed: 3.1ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3831\n",
            "\n",
            "0: 384x640 1 child, 3 therapists, 16.7ms\n",
            "Speed: 8.4ms preprocess, 16.7ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3832\n",
            "\n",
            "0: 384x640 1 child, 2 therapists, 10.8ms\n",
            "Speed: 3.0ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3833\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.2ms\n",
            "Speed: 2.9ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3834\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 11.5ms\n",
            "Speed: 3.5ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3835\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.7ms\n",
            "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3836\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.5ms\n",
            "Speed: 3.0ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3837\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 12.0ms\n",
            "Speed: 2.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3838\n",
            "\n",
            "0: 384x640 3 childs, 2 therapists, 10.6ms\n",
            "Speed: 3.2ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3839\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 11.3ms\n",
            "Speed: 2.5ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3840\n",
            "\n",
            "0: 384x640 2 childs, 2 therapists, 10.9ms\n",
            "Speed: 4.1ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3841\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 18.1ms\n",
            "Speed: 9.7ms preprocess, 18.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3842\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 21.7ms\n",
            "Speed: 4.2ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3843\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 9.9ms\n",
            "Speed: 10.7ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3844\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 11.5ms\n",
            "Speed: 4.8ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processing frame 3845\n",
            "\n",
            "0: 384x640 2 childs, 1 therapist, 12.5ms\n",
            "Speed: 3.2ms preprocess, 12.5ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "No more frames to process or error reading frame\n",
            "Video processing complete. Output video saved.\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Define paths\n",
        "    yolo_model_path = \"/content/best_model___.pt\"\n",
        "    reid_model_path = \"/content/resnet50_fc512_duke_xent.pt\"\n",
        "    input_video_path = \"/content/video.mp4\"\n",
        "    output_video_path = \"/content/video__new_result.mp4\"\n",
        "\n",
        "    # Initialize models\n",
        "    yolo_model, tracker = initialize_models(yolo_model_path, reid_model_path)\n",
        "\n",
        "    # Get class names and IDs of interest\n",
        "    class_names_dict = yolo_model.model.names  # This will contain the updated class names, like 'therapist' and 'child'\n",
        "    classes_of_interest = ['therapist', 'child']\n",
        "    class_ids = [class_id for class_id, class_name in class_names_dict.items() if class_name in classes_of_interest]\n",
        "\n",
        "    print(\"Class IDs of interest:\", class_ids)\n",
        "\n",
        "    # Process and save video\n",
        "    save_video(input_video_path, output_video_path, yolo_model, tracker, class_ids, class_names_dict)\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BUBDy--cUnh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
